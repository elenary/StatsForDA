[["данные-и-типы-шкал.html", "3 Данные и типы шкал 3.1 Измерение и переменная 3.2 Количественные и неколичественные данные 3.3 Типы шкал 3.4 Непрерывные и дискретные данные", " 3 Данные и типы шкал 3.1 Измерение и переменная Что значит измерить что-либо? Это значит, привести в соответствие исследуемому признаку какое-либо значение на шкале. Что может примером признака? Что угодно, что нам нужно измерить в рамках исследования: количество кружек кофе в день, уровень концентрации, количество ошибок, время реакции, степень выгорания, количество выполненных задач, уровень нейротизма, рейтинг студента, количество детей в семье, температура и т.д. Признак, который мы исследуем, по-другому называется переменной. С этим понятием мы будем сталкиваться постоянно в анализе данных. По сути, если мы посмотрим на табличку наблюдений, то любой столбец с измерениям – это переменная. По строкам располагаются наблюдения, например, каждый новый человек из нашей выборки. Значение в определенной колонке – это значение переменной данного наблюдения. Вернемся к кейсу с Никитой, которые изучает выгорание сотрудников ВУЗов. Сделаем табличку данных, которые мог намерить Никита. library(tidyverse) library(kableExtra) teacher_number &lt;- seq(1,30,1) age &lt;- sample(22:60, size = 30, replace = T) exp_years &lt;- sample(1:8, size = 30, replace = T) exp_scaled &lt;- ifelse(exp_years &gt;= 1 &amp; exp_years &lt;= 2, &quot;от 1 до 2&quot;, ifelse(exp_years &gt; 2 &amp; exp_years &lt;= 5, &quot;от 3 до 5&quot;, ifelse(exp_years &gt; 5, &quot;больше 5&quot;, exp_years))) burnout_MBI &lt;- sample(19:70, size = 30, replace = T) univer &lt;- rep(c(&quot;MSU&quot;, &quot;HSE&quot;, &quot;MSU&quot;, &quot;RANEPA&quot;, &quot;HSE&quot;, &quot;RANEPA&quot;),5) burnout &lt;- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer) kable(burnout[1:10,]) teacher_number age exp_years exp_scaled burnout_MBI univer 1 30 1 от 1 до 2 50 MSU 2 24 8 больше 5 65 HSE 3 55 7 больше 5 66 MSU 4 28 7 больше 5 22 RANEPA 5 37 8 больше 5 33 HSE 6 44 4 от 3 до 5 57 RANEPA 7 58 6 больше 5 70 MSU 8 37 3 от 3 до 5 33 HSE 9 35 1 от 1 до 2 30 MSU 10 57 5 от 3 до 5 25 RANEPA Что здесь будет являться переменными? В определении измерения помимо признака есть второе важное понятие – это шкала. Шкала – это система измерения. Для того, чтобы мы все могли пользоваться одинаковыми единицами измерениями и не сходили с ума, мы, люди планеты Земля, пользуемся едиными шкалами. Всего их 4, и они бывают метрические и неметрические – то есть, можем ли мы приложить измерительную линейку к ним или нет. В качестве измерительной линейки здесь имеется в виду любой условный прибор, в котором есть цена деления (сантиметр, грамм, секунда, штука). 3.2 Количественные и неколичественные данные Данные, измеренные метрическими шкалами – это количественные данные (например, рост, вес, число заболевших, температура. То, что нельзя измерить метрическими шкалами (например, цвет глаз, самочувствие, уровень нейротизма, уровень образования) – это неколичественные данные, которые могут носить разные названия: категориальные, иногда качественные. Иногда данные называют качественными в противоположность количественным, но это не совсем верно: разделение на количественные и качественные обычно применяется по отношению к типам исследований, где качественные исследования – это, например, интервью или анализ блоков текста. Но в результате этого анализа у нас вполне могут получиться количественные переменные, например, количество раз, которые употреблялось то или иное слово, поэтому по отношению к данным, а не типам исследований, я рекомендую не использовать слово “качественные”. С количественными данными попроще – это все, что можно измерить метрической шкалой, условной линейкой. Среди неколичественных данных встречаются два типа: категориальные и ранговые (порядковые) Эти данные мы подробно рассмотрим на шкалах. 3.3 Типы шкал Как мы уже поняли, разные данные относятся к разным шкалам. Разные шкалы обладают разной измерительной мощностью – точностью, с которой мы измеряем признак. Один и тот же признак можно измерить с разной точностью: например, в зависимости от исследовательского вопроса, рост может быть выражен количественно в сантиметрах на интервале {0; ∞}, а может быть закодирован в виде {“меньше 150 см”; 150 см и больше}, если нас интересует только преобладание над определенной чертой. Всего существуют 4 шкалы, если располагать их снизу вверх по измерительной мощности: наименований, порядковая, интервальная, отношений. Шкала Описание Возможные операции Примеры Отношений (абсолютная) Количественная, есть абсолютный ноль, можно посчитать и на сколько больше или меньше, и во сколько раз =, \\(\\neq\\), &gt;, &lt;, +, -, ×, ÷ Рост, вес, число заболевших Интервальная (разности) Количественная, но нет абсоолютного нуля, можно посчитать на сколько больше или меньше, но нельзя посчитать, во сколько раз =, \\(\\neq\\), &gt;, &lt;, +, - Температура в градусах Цельсия, времяисчисление по разным календарям Порядковая (ранговая, ординальная) Категориальная (качественная), можно установить “больше” или “меньше”, но нельзя посчитать количественно, на сколько больше или меньше =, \\(\\neq\\), &gt;, &lt; Уровень образования, уровень нейротизма, спортивный рейтинг Наименований (номинальная) Категориальная (качественная), нельзя установить “больше” или “меньше” =, \\(\\neq\\) Пол, цвет, место жительства, название университета 3.4 Непрерывные и дискретные данные Видим, что самые богатые возможности для измерений у нас простираются в количественных шкалах – на шкале отношений (она самая крутая) и интервальной шкале (она похуже и вообще, на самом деле, в исследованиях в нашей области встречается редко). Количественные данные бывают дискретные, когда переменная принимает строго определенные значения, и непрерывные, когда может принимать какие угодно значения, до бесконечности или на заданном интервале. Например, в нашем примере с исследованием выгорания переменная возраст (age) может принимать любые значения: преподаватели могут быть и возраста 25 лет, и 27.5 лет, и 31.666.. лет – это все значения из области допустимых значений для этой переменной. Но если мы рассмотрим количество заболевших коронавирусом, то их никак не может быть 27.5 или 31.666.. – заболевшие не выражаются в дробных долях от одного человека. Важное понятие здесь – область допустимых значений. У непрерывной переменной это всегда интервал, например {0;+∞}, у дисретных – строго определенные значения, которые, тем не менее, могут тоже стремиться к бесконечности, например, {0;1;2;3;4;5;6;7;8…} "],["вероятность-и-случайные-величины.html", "4 Вероятность и случайные величины 4.1 Случайная величина 4.2 Вероятность 4.3 Дискретные СВ и статистическое определение вероятности 4.4 Столбчатая диаграмма 4.5 Непрерывные СВ и геометрическое определение вероятности 4.6 График плотности вероятности и гистограмма", " 4 Вероятность и случайные величины 4.1 Случайная величина Мы немного поговорили про переменные, они же – признаки исследуего нами объекта (рост, вес, пол, уровень образования и так далее). Поговорили, что чтобы измерить признак, нужно привести в соответствие какое-либо значение из шкалы. Теперь давайте посмотрим на математический смысл переменных и их значений. С математической точки зрения значения переменных являются случайными величинами. В теории вероятностей случайной величиной называется величина, которая в данный момент времени (момент измерения) может принимать только одно значение, которое нельзя предугадать точно (до измерения). Нам нужно замерить рост? Можем ли мы заранее сказать, сколько точно вплоть до микрометров он будет составлять? Если нет, значит, мы провели испытание (статистический термин единичного исследования или измерения), в ходе которого случайная величина рост приняла определенное значение (мы привели ей в соответствие какое-то значение из количественной шкалы). Говоря статистическим языком, наступило событие или явление “РОСТ = 178 см”. Нужно определить время реакции после выпитой кружки кофе? Все то же самое, время реакции – случайная величина. Результат прохождения опросника – случайная величина. Название ВУЗа, из которого пришел наш испытуемый – случайная величина. Это понятие нужно нам для того, чтобы мы могли считать вероятности наступления определенных событий, то есть наших измеренных переменных. Дело в том, что про вероятности случайных величин нам плюс-минус понятно и просто, а вот если величина перестает быть случайной, расчет вероятности становится сложнее. И мы пока рассматриваем только случайные величины. 4.2 Вероятность Что такое вероятность? По сути, вероятность – это численно выраженая возможность наступления того или иного события. Вероятность может рассматриваться как частота наступления уже свершившегося события: по тому, как часто оно происходило, можно оценить, какова вероятность его наступления в дальнейшем. Здесь начинается неожиданная развилка: в зависимости от того, как мы понимаем вероятность, приравниваем ли ее к частоте, статистика делится на байесовскую (bayesian) и частотную (фреквинтистскую, frequentist). Статистика, в которой мы заменяем вероятности частотами, а не высчитываем вероятность по сложной формуле, и в которой мы будем работать – частотная (https://en.wikipedia.org/wiki/Frequentist_inference). То есть говоря о вероятности, мы будем понимать ее исключительно так же, как и частоту, забываем про существование условной вероятности, формулу полной вероятности и других сложных концепций: если бы мы провели много раз одно и то же исследование, скажем, тысячу, и посмотрели, сколько раз в этом исследовании выпадает результат, который нас интересует, мы бы сказали, что вероятность наступления этого события – это сколько раз оно выпадало из тысячи. И будем руководствоваться только этим смыслом. Есть разные определения, в рамках статистики различают статистическое и геометрическое определение вероятностей. Можно углубиться на http://mathprofi.ru/sluchainaya_velichina.html. Разные определения вероятности используются в зависимости от того, с какими случайными величинами (СВ) мы работаем, различают два их типа: * дискретные СВ * непрерывные СВ 4.3 Дискретные СВ и статистическое определение вероятности Самая часто используемая в теории вероятностей модель – бросание обычного шестигранного игрального кубика. Кубик отличает то, что всего возможно наступление 6 событий (не будем рассматривать, что кубик смещенный или какой-то неправильный). То есть область допустимых значений или область определения для случайной величины “бросок кубика” это 6 значений: $ D {1, 2, 3, 4, 5, 6}$ Величина, область значений которой состоит из конечного числа натуральных чисел (1; 2; 3; 4; 5…), называется дискретной. Для определения вероятности дискретной величны воспользуемся ее статистическим определением, не углубляясь в статистические термины: если при проведении испытания возможны \\(n\\) равновероятных исходов значений случайной величины \\(A\\), при этом в \\(m\\) из них случается интересующее нас конкретное событие \\(A_{i}\\),то вероятность наступления события \\(P(A_{i}) = \\frac{m}{n}\\) Бросание игрального кубика – это испытание, выпадение одной из граней – исход, а выпадение конкретно шестерки – событие. Обозначим выпадние грани в результате бросания кубика буквой \\(K\\). Чему равна вероятность выпадения каждой грани \\(K_{1}\\), \\(K_{2}\\), \\(K_{3}\\), \\(K_{4}\\), \\(K_{5}\\), \\(K_{6}\\)? По статистическому определению вероятности: возможных исходов всего – 6, интересующее нас событие случается в одном случае из 6, то есть: \\(P(K_{1})\\) = \\(P(K_{2})\\) = \\(P(K_{3})\\) = \\(P(K_{4})\\) = \\(P(K_{5})\\) = \\(P(K_{6})\\) = \\(\\frac{1}{6}\\) Важно, что все события равновероятны. Если бы мы жили в мире с кубиками со смещенным центром тяжести, выпадние граней не было бы равновероятным. А чему равна полная вероятность? Как можно вывести это математически из вероятностей наступления событий в бросании игрального кубика? \\(P(K_{1})\\) + \\(P(K_{2})\\) + \\(P(K_{3})\\) + \\(P(K_{4})\\) + \\(P(K_{5})\\) + \\(P(K_{6})\\) = \\(1\\) Единичные события нас мало интересуют (предмет изучения теории вероятностей – массовые события), поэтому давайте представим, что мы бросили кубик несколько раз. Например, 20. library(TeachingDemos) rolls &lt;- dice(20,1) t(rolls) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14] ## Red 2 4 6 1 2 3 4 1 1 6 3 2 4 3 ## [,15] [,16] [,17] [,18] [,19] [,20] ## Red 1 1 6 2 3 5 4.4 Столбчатая диаграмма Построим таблицу частот выпадения каждой грани table(rolls) ## rolls ## 1 2 3 4 5 6 ## 5 4 4 3 1 3 prop.table(table(rolls)) ## rolls ## 1 2 3 4 5 6 ## 0.25 0.20 0.20 0.15 0.05 0.15 Можем наглядно посмотреть это на столбчатой (столбиковой) диаграмме – графике, отображающем частоты встречаемости событий дискретной переменной. rolls %&gt;% ggplot(aes(x=Red)) + geom_histogram(bins = 6, binwidth = 1) + theme_minimal() Гистограмма – это частный случай столбчатой (или столбиковой) диаграммы. Она отличается от столбчатой диаграммы тем, что, в отличие от нее, мы сами можем регулировать ширину столбика по оси \\(x\\), задавать, сколько значений переменной х в него войдет, и высотка столбика будут пересчитана в зависимости от этого количества. В столбиковой диаграмме мы так сделать не можем, по оси \\(x\\) всегда только одно значение, каждый столбик соотносится с конкретным значением переменной. 4.5 Непрерывные СВ и геометрическое определение вероятности С непрерывными величинами чуть посложнее, поэтому нам понадобится геометрическое определение вероятности. Построим отрезок на оси \\(x\\) от 0 до 1. Допустим, мы проводим испытание, где бесконечно малым курсором проводим по этому отрезку. Остановку курсора в какой-то точке обозначим за \\(L\\). Какова вероятность, что курсор остановится в точке с координатами \\(x=0.4857856\\)? Сколько точек лежат в этом отрезке? Вспоминаем, что точка – бесконечно малая величина, поэтому на этом отрезке лежит бесконечное число точек. Область допустимых значений или область определения для случайной величины “остановка курсора в точке” – бесконечное число значений в диапазоне \\(D \\in [0;1]\\) Величина, область допустиимых значений которой состоит из бесконечного числа значений на каком-либо ненеулевом промежутке, которые невозможно посчитать, называется непрерывной. Попробуем воспользоваться статистическим определением вероятности: \\(P(L_{.4857856}) = \\frac{1}{множество всех точек на отрезке} = \\frac{1}{\\infty} \\sim 0\\) В пределе это число равно 0. Получается, что мы не можем посчитать математически (статистически), и приходится прибегать к геометрическому определению. 4.6 График плотности вероятности и гистограмма Построим таблицу и график частот для значений из отрезка. Для это сгенерируем набор из 10 случайных чисел в диапазоне [0;1]. set.seed(42) line_sample &lt;- runif(10, min=0, max=1) line_sample ## [1] 0.9148060 0.9370754 0.2861395 0.8304476 0.6417455 0.5190959 0.7365883 ## [8] 0.1346666 0.6569923 0.7050648 table(line_sample) ## line_sample ## 0.13466659723781 0.286139534786344 0.519095949130133 0.641745518893003 ## 1 1 1 1 ## 0.656992290401831 0.705064784036949 0.736588314641267 0.830447626067325 ## 1 1 1 1 ## 0.914806043496355 0.937075413297862 ## 1 1 prop.table(table(line_sample)) ## line_sample ## 0.13466659723781 0.286139534786344 0.519095949130133 0.641745518893003 ## 0.1 0.1 0.1 0.1 ## 0.656992290401831 0.705064784036949 0.736588314641267 0.830447626067325 ## 0.1 0.1 0.1 0.1 ## 0.914806043496355 0.937075413297862 ## 0.1 0.1 Видим, что все значения встретились только один раз. Построим график. Можем построить гистограмму для этих значений: as_tibble(line_sample) %&gt;% ggplot(aes(x=value)) + geom_histogram(bins = 10) + theme_minimal() Получается довольно странно: мы видели, что частота встречаемости разных значений – всегда была единичка, а на гистограмме кажется по-другому. Так происходит, потому что мы взяли только 10 очень конкретных крошечных значений из непрерывной величины, и нам попались какие-то значения – рядом друг с другом, и столбики для их частот “слиплись” в один большой столбик, и частота получилась не 1, а 2. А какие-то значения попались далеко друг от друга, поэтому их столбики слиплись не друг с другом, а с более близкими к ним столбиками, и на месте частоты для этого значения образовалась дырка. Такой график для непрерывной величины не очень верный: чтобы стобики не “слипались” и не обманывали нас визуально, будто где-то в значениях есть дыра, для непрерывных величин мы будем использовать другой график – график плотности вероятности (probability density) as_tibble(line_sample) %&gt;% ggplot(aes(x=value)) + geom_density() + theme_minimal() Он тоже показывает так, будто какие-то значения более частые, какие-то менее – но ситуация уже гораздо лучше. Здесь мы сгенерировали 10 значений, а если нам вдруг нужно визуализировать все возможные значения из отрезка [0;1], а х бесконечность? Тогда гистограмма вообще потеряет всякий смысл, так как разбивает все значения на столбики из конечных интервалов, а вот непрерывная линию будет подходящей – в непрерывной линии бесконечное число значений! То есть выходит следующая логика: для визуализации дискретных величин обычно используют гистограмму, для визуализации непрерывных – график плотности вероятности. Почему плотности вероятности, а не самой вероятности? Потому что как мы вывели выше, если бы мы считали точно именно вероятность, она бы стремилась к нулю. А засчет того, что это именно плотность, буквально, “тут побольше значений”, “тут поменьше значений” – получается осмысленный график. То есть, простыми словами, плотность вероятности – характер распределения вероятностей в самом значении и его окрестности. Если в интервале [0.25;0.35] оказалось (чисто случайно) мало значений, а в интервале [0.58;0.65] оказалось побольше – то на графике плотности вероятности мы увидим различия в вероятности для этих интервалов, хотя вероятность встретить каждое конкретное очень точно заданное число (например, 0.4857856) стремится к 0. Мы не будем рассматривать математически смысл плотности вероятности, но функция плотности вероятности является производной от функции самой вероятности. Попробуем сгенерировать этот же график для 500 и 10 000 точек и посмотреть, к чему будет стремиться график плотности вероятности в случае, когда мы случайно генерируем множество значений, все из которых равновероятны? line_sample_500 &lt;- runif(500, min=0, max=1) as_tibble(line_sample_500) %&gt;% ggplot(aes(x=value)) + geom_density() + theme_minimal() 10000 точек из этого же диапазона line_sample_10000 &lt;- runif(10000, min=0, max=1) as_tibble(line_sample_10000) %&gt;% ggplot(aes(x=value)) + geom_density() + theme_minimal() Итак, запомнили, что мы рассматриваем признаки (переменные) как случайные величины, они могут быть дисркетные и непрерывные, и их вероятности описываются определенными законами распределения. "],["распределения.html", "5 Распределения 5.1 Виды распределений 5.2 Функции распределения", " 5 Распределения Что такое распределение? Мы постоянно употребляем это слово. Говоря о распределениях, мы имеем в виду закон распределения случайной величины – соответствие между возможными значениями этой величины из области ее допустимых значений и: вероятностями этих значений – для дискретной величины, либо плотностью вероятности – для непрерывной величины. 5.1 Виды распределений Законы распределений вероятностей можно вывести совсем не для всех величин! Но так сложилось, что некоторые закономерности, распределения вероятностей мы можем описать формулами, как в физике: сила тяжести (сила, с которой Земля притягивает все теля) прямо пропорциональная массе объекта, взятая с коэффициентом ускорения свободного падения \\(F = m*g\\). Мы это заметили относительно окружающего мира и вывели закон (не лично мы, а вообще представители планеты Земля). Точно так же и с распределниями: мы заметили, что вероятности распределения некоторых случайных величин подчиняются определенным законам, и записали их. Например, доказано, что непрерывные случайные величины, на которые действует множество случайных факторов (например, рост, вес и тд), распределяются в соответствии с распределением Гаусса, оно же – нормальное распределение. Его формула: \\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\) plot(dnorm(1:1000, mean = 500, sd = 100)) Или, например, экспоненциальное распределение: \\(P(x)= \\lambda \\times e^{-\\lambda x}\\) plot(dexp(x = 1:100, rate = 0.1)) χ2-распределение: \\(P(x)= \\lambda \\times e^{-\\lambda x}\\) plot(dchisq(1:1000, df=100)) Или, например, биномиальное распределение: \\(P(k | n, p) = \\frac{n!}{k!(n-k)!} \\times p^k \\times (1-p)^{n-k} = {n \\choose k} \\times p^k \\times (1-p)^{n-k}\\) plot(dbinom(1:100, size=100, prob=0.5)) Посмотреть и ужаснуться можно тут http://www.math.wm.edu/~leemis/chart/UDR/UDR.html. Нам, к счастью, ничего из этого не понадобится. Менее пугающая версия https://www.johndcook.com/blog/distribution_chart/#normal На примере данных про выгорание: kable(burnout) teacher_number age exp_years exp_scaled burnout_MBI univer 1 30 1 от 1 до 2 50 MSU 2 24 8 больше 5 65 HSE 3 55 7 больше 5 66 MSU 4 28 7 больше 5 22 RANEPA 5 37 8 больше 5 33 HSE 6 44 4 от 3 до 5 57 RANEPA 7 58 6 больше 5 70 MSU 8 37 3 от 3 до 5 33 HSE 9 35 1 от 1 до 2 30 MSU 10 57 5 от 3 до 5 25 RANEPA 11 30 4 от 3 до 5 19 HSE 12 24 3 от 3 до 5 42 RANEPA 13 22 7 больше 5 35 MSU 14 56 2 от 1 до 2 32 HSE 15 50 4 от 3 до 5 70 MSU 16 50 7 больше 5 32 RANEPA 17 46 2 от 1 до 2 45 HSE 18 47 4 от 3 до 5 33 RANEPA 19 26 3 от 3 до 5 29 MSU 20 53 1 от 1 до 2 31 HSE 21 53 7 больше 5 19 MSU 22 41 6 больше 5 20 RANEPA 23 59 6 больше 5 62 HSE 24 38 7 больше 5 68 RANEPA 25 55 2 от 1 до 2 33 MSU 26 42 8 больше 5 23 HSE 27 27 3 от 3 до 5 50 MSU 28 31 1 от 1 до 2 45 RANEPA 29 54 3 от 3 до 5 49 HSE 30 56 5 от 3 до 5 32 RANEPA burnout %&gt;% ggplot(aes(x=exp_years)) + geom_histogram(binwidth = 0.5) + theme_minimal() burnout %&gt;% ggplot(aes(x=age)) + geom_density() + theme_minimal() 5.2 Функции распределения Выше мы познакомились с плотностью вероятностью, зафиксируем все применимые к вероятности функции, которые бывают полезны: функция плотности вероятности (probability density function) для непрерывных СВ (например, рост, вес) и функция вероятности (probability mass function) для дискретных СВ (например, количество заболевших) – самая простая базовая функция, часто обозначается буквой d* функция накопленной вероятности / плотности вероятности (cumulative distribution function) для непреревных СВ, часто обозначается буквой p* квантильная функция (quantile function), она же обратная функция накопленной плотности распределения, об этом попозже, часто обозначается буквой q* Функция плотности вероятности (cumulative distribution function; cdf) Зачем они нужны? Разберемся на примере тестов на IQ. Функция плотности вероятности (probability density function) iq &lt;- seq(50,150, 0.1) plot(iq, dnorm(iq, mean = 100, sd = 15)) Функция накопленной плотности (cumulative distribution function; cdf) plot(iq, pnorm(iq, mean = 100, sd = 15)) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
