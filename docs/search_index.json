[["stats_criteria.html", "9 Статистические критерии 9.1 Степени свободы 9.2 Ключевые распределения и статистики критериев (Z, T, F) 9.3 Выбор статистического критерия 9.4 T-тест (сравнение средних двух групп) 9.5 ANOVA (дисперсионный анализ)", " 9 Статистические критерии Статистическим критерием называется правило, по которому мы будем пытаться отвергнуть нулевую гипотезу. Разным гипотезам и данным подходят разные критерии. Статистический критерий включает: Теоретическое распределение, на графике плотности вероятности которого мы будем располагать сравниваемые средние: вид кривой и математическая формула этого вероятностного закона (Т-распределение, F-распределение, \\(\\chi^2\\)-распределение и другие) Формулу, по которой мы будем рассчитывать искомое значение критерия (как уже рассчитывали \\(Z_{Mмаленькие}\\)) исходя из наших данных на выборке для Z-оценки) Cоответствие каждого посчитанного по этой формуле значения (например, Z-значения или Т-значения) проценту данных, расположенных за этим значением на графике плотности вероятности этого распределения (мы не будем пользоваться такими таблицами соответствий, за нас все считают бесдушные машины) Формулу для рассчета числа степеней свободы для этого критерия 9.1 Степени свободы Число степененй свободы (Degrees of Freedom, df) – количество направления для изменения признака. Формула для расчета степеней свободы своя для каждого статистического критерия (нам не нужно рассчитывать самостоятельно) и зависит, собственно, от вида статистического критерия, дизайна нашего исследования (сколько сравнений мы проводим) и числа наблюдений в выборке. Для t-критерия число степеней свободы высчитывается очень просто: \\(df = n_1 -1 + n_2 - 1 = n_1 + n_2 - 2\\) 9.2 Ключевые распределения и статистики критериев (Z, T, F) Z-распределение В прошлый раз мы рассмотрели пример, когда сравнивали среднее время, которое проводят родители с детьми, в больших и маленьких городах, и сравнивали эти значения на примере [Z-распределения] {stat_test_example): мы получали Z-статистику и находили соответствующий распределению Z-оценок процент данных для найденной точки. \\(Z = \\frac{M - \\mu}{\\frac{\\sigma}{\\sqrt{n}}} = \\frac{M - \\mu}{\\frac{sd}{\\sqrt{n}}}\\) Это самый простой вариант критерия для проверки статистических гипотез – но не единственный. На самом деле, в качестве распределения, на которое мы помещаем точки (средние) и сравниваем их друг с другом, могут быть и другие, в зависимости от характера наших данных. Рассмотрим основные из них. https://gallery.shinyapps.io/dist_calc/ T-распределение Стьюдента Как мы видим, оно очень похоже на Z-распределение, но у него чуть более приподнятые хвосты. Формула, которой задается вероятностный закон Т-распределения Стьюдента: Негласно считается, что при объеме выборки от n=30 t-распределение считается близким к Z-распределению, которе является нормальным, и можем использовать его. Но в реальности это не совсем так. Формула для расчета T-значения исходя из наших данных: \\(t = \\frac{M_1 - M_2}{\\frac{sd_{1;2}}{\\sqrt{n_{1;2}}}} = \\frac{M_1 - M_2}{\\sqrt{\\frac{sd_1^2}{n_1} + \\frac{sd_2^2}{n_2}}}\\) F-распределение \\(\\chi^2\\)-распределение 9.3 Выбор статистического критерия Вопросы, которые влияют на выбор статистического теста: Зависимая переменная: количественная (интервальная шкала или шкала отношений) или категориальная (номинативная или порядковая шкала)? Если ЗП количественная – она описывается параметрическим (чаще всего нормальным) распределением? (определяется чаще “на глаз” по графику плотности вероятности или QQ-графиками, тесты на нормальность почти всегда будут давать негативный результат из-за чувствительности и поэтому в современном анализе данных используются мало – подробнее чуть ниже) Сколько независимых переменных? НП количественные (интервальная и шкала отношений) или категориальные (номинативная и порядковая)? Если НП категориальные и мы сравниваем группы – данные в группах зависимы или нет? Если нет, как сильно отличаются данные в группах, можем ли сказать, что дисперсия ЗП примерно одинакова в группах или нет? (равенство дисперсий называется Homogeneity of Variance, проверяется с помощью Levene’s test) Есть большое количество схем, но обычно они сильно перегруженны и сложны в использовании, больше путают, чем помогают. Но я зачем-то все равно решила попробовать нарисовать свою, и вот результат: https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470 (будет обновляться) 9.3.1 Параметрические и непараметрические критерии В списке вопросов, которые влияют на выбор статистического критерия, вторым пунктом идет вопрос о параметрике. Что это значит? Под параметрическими распределением имеется в виду любое распределение, которое можно описать законом. Вспоминаем: закон распределения – это формула, по которой можем привести любому числу вероятность встретить такое значение в природе (разбирали это в теме про распределения ). В этой формуле есть параметры – неизвестные переменные, которые мы стремимся узнать. Для нормально распределения это среднее генерально совокупности (математическое ожидание) и стандартное отлокнение генеральной совокупности. Посмотрим еще раз на формулу нормального распределения и найдем их: \\(P(x) = \\frac{e^{-(x - \\mu)^{2}/(2\\sigma^{2}) }} {\\sigma\\sqrt{2\\pi}}\\) Вспомним еще, что признаки, которые мы исследуем в психологических исследованиях, являются в основном случайными величинами, а при большом объеме данных случайные величины, согласно центральной предельной теореме, распределены нормально. Поэтому в исследования психологических признаков под параметрическим распределением обычно имеется в виду одно конкретное распределение – нормальное. Если переменная, которая выражает исследуемый признак, распределена нормально, то мы можем применять для ее изучения параметрические статистические критерии, предполагающие нормальное распределение данных: т-тест, ANOVA. Если переменная распределна не нормально, то нужно использовать их непараметрические аналоги: тест Манна-Уитни или Вилкоксона, тест Краскела-Уоллиса. Проверка на нормальность – довольно устоявшаяся в алгоритме проверки гипотез рутина. Но есть один нюанс. При малом объеме данных сложно понять, как распределены данные, нормально или нет. При большой объеме данных любое, даже малейшее отклонение от нормального распределения будет значимым. Поэтому в современной культуре исследований проверке на нормальность уже не отводится такое сакральное значение. Важнее, например, оперировать размером выборки: если она достаточно большая (можем вернуться к эмприческому правилу к n &gt;= 30), то достаточно взлянуть на распределение данных и QQ-plot, и если они не вызывают подозрений, смело использовать параметрические методы. Если выборка маленькая или если данные при большой выборке явно сильно отличаются от колоколообразной гауссианы, то лучше использовать a) непараметрические аналоги б) обобщенные линейные модели (про них поговорим позже). Итак, я бы не рекомендовала делать тщательную проверку на нормальность вроде теста Колмогорова-Смирноова или Шапиро-Улика (хотя можно их считать, это не возбраняется), а оценить принадлежность к параметрическому (в данном случае нормальному) распределению следующим образом: График плотности вероятности или гистограмма: симметричность и эксцесс обсуждали их тут QQ-plot Графики из книжки Энди Филда “Discovering Statistics Using R” Верхняя строка графиков показывает, как могут выглядить графики в случае близкого к норрмальному распределению: гистограмма или график плотности вероятности зависимой переменной похож на колоколообразную гауссиану, на QQ-плоте теоретические квартили почти соответствую фактическим на данных – чем больше этот график похож на прямую линию y = x, тем лучше. Нижняя строка графиков показывает обратную историю, как могут выглядить визуализации распределения ЗП, которую мы НЕ будем считать нормально распределенной: график плотности и вероятности и гистограмма сильно скошенны влево, QQ-плот уже мало похож на прямую. 9.3.2 Зависимые и независимые выборки Еще один пункт при выборе статистического критерия – это зависимые у нас наблюдения или нет. Мы обсуждали зависимые и независимые выборки вначале. Теперь это знание пригодится нам, чтобы правильно подбирать статистический метод. 9.4 T-тест (сравнение средних двух групп) Первым из статистических критериев мы рассмотрим один из самых простых вариантов – т-тест. Это статистический критерий, распределение которого относится к семейству Т-распределений – очень похоже на нормальное, но с более приподнятыми хвостами. Используется для сравнения средних двух групп, измеренных в метрической (количественной) шкале. Для остальных шкал т-тест не подходит (хотя признаки, измеренные по шкале Лайкерта, например, по шкале от 1 до 5, иногда могут приписываться к количественным измерениям, но в рамках этого курса мы не будем это затрагивать) Нулевая и альтернативная гипотезы: \\(H_0\\): \\(\\mu_1 = \\mu_2\\) \\(H_1\\): \\(\\mu_1 \\neq \\mu_2\\) Вернемся к данным про студентов и нашим вопросам и разберем теперь следующий вопрос: Отличается ли статистически значимо средний балл по математике у тех, кто чаще или реже пропускает занятия? student school sex age address famsize Pstatus Medu Fedu Mjob Fjob reason guardian traveltime studytime failures schoolsup famsup paid_mat activities nursery higher internet romantic famrel freetime goout Dalc Walc health absences_mat G1_mat G2_mat G3_mat paid_por absences_por G1_por G2_por G3_por G_mat G_por absences_mat_groups absences_por_groups id1 GP F 18 U GT3 A 4 4 at_home teacher course mother 2 2 0 yes no no no yes yes no no 4 3 4 1 1 3 6 5 6 6 no 4 0 11 11 5.666667 7.333333 middle less id2 GP F 17 U GT3 T 1 1 at_home other course father 1 2 0 no yes no no no yes yes no 5 3 3 1 1 3 4 5 5 6 no 2 9 11 11 5.333333 10.333333 less less id4 GP F 15 U GT3 T 4 2 health services home mother 1 3 0 no yes yes yes yes yes yes yes 3 2 2 1 1 5 2 15 14 15 no 0 14 14 14 14.666667 14.000000 less less id5 GP F 16 U GT3 T 3 3 other other home father 1 2 0 no yes yes no yes yes no no 4 3 2 1 2 5 4 6 10 10 no 0 11 13 13 8.666667 12.333333 less less id6 GP M 16 U LE3 T 4 3 services other reputation mother 1 2 0 no yes yes yes yes yes yes no 5 4 2 1 2 5 10 15 15 15 no 6 12 12 13 15.000000 12.333333 middle middle id7 GP M 16 U LE3 T 2 2 other other home mother 1 2 0 no no no no yes yes yes no 4 4 4 1 1 3 0 12 12 11 no 0 13 12 13 11.666667 12.666667 less less id8 GP F 17 U GT3 A 4 4 other teacher home mother 2 2 0 yes yes no no yes yes no no 4 1 4 1 1 1 6 6 5 6 no 2 10 13 13 5.666667 12.000000 middle less id9 GP M 15 U LE3 A 3 2 services other home mother 1 2 0 no yes yes no yes yes yes no 4 2 2 1 1 1 0 16 18 19 no 0 15 16 17 17.666667 16.000000 less less id10 GP M 15 U GT3 T 3 4 other other home mother 1 2 0 no yes yes yes yes yes yes no 5 5 1 1 1 5 0 14 15 15 no 0 12 12 13 14.666667 12.333333 less less id11 GP F 15 U GT3 T 4 4 teacher health reputation mother 1 2 0 no yes yes no yes yes yes no 3 3 3 1 2 2 0 10 8 9 no 2 14 14 14 9.000000 14.000000 less less Мы уже обсуждали в предыдущей главе, что ЗП здесь – средний балл по математике (колонка G_mat), НП – количество пропусков занятий по этом предмету (absences_mat). И ЗП, и НП – количественные, закодированы в шкале отношений. Значит, можем пользоваться той веткой статистических тестов, которая подходит для количественных ЗП. Пройдем по алгоритму выбора статистического теста: https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470 Помимо основных отраженных ответвлений, видим, что для каждого критерия существует ряд допущений. Допущения (assumptions) – это утверждения про характер наших данных, без соблюдения которых параметрические тесты будут работать некорректно (поэтому их никто и не любит). 9.4.1 Допущения для т-теста Данные распределены нормально (или, если измеряемый признак является случайной велиичной, в группах больше 30 наблюдений) – обсуждали эту проверку здесь Дисперсии однородны (гомогенны) – проверяется с помощью теста Ливеня (Homogeneity of Variance test, он же Levene’s test). 9.4.2 Непараметрические аналоги В случае, если допущения нормальности и однородности дисперсий нарушаются, мы не можем использовать т-тест. 9.4.3 Независимые и парные тесты Тут тоже все довольно просто: Если у нас независимые выборки, то мы используем независмый т-тест или его непараметрический аналог – тест Манна-Уитни. Если выборки зависимые, то мы используем парный т-тест или его непараметрический аналог – тест Вилкоксона. Обилие наименований может быть пугающим, но по сути, это один и тот же тест с небольшими поправками – просто в разных названиях увековечено больше статистиков! 9.4.4 Вычисление т-теста и непараметрических аналогов Средний балл и стандартное отклонение в группах тех, кто прогуливает меньше всего (на паре мы назвали их прихожанами): ## # A tibble: 1 x 3 ## mean sd n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 11.2 3.70 207 Средний балл в группах прогулищиков: ## # A tibble: 1 x 3 ## mean sd n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 10.2 3.52 22 students %&gt;% filter(absences_mat_groups != &quot;middle&quot;) -&gt; students_2 t.test(students_2$G_mat ~ students_2$absences_mat_groups, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: students_2$G_mat by students_2$absences_mat_groups ## t = 1.2839, df = 26.204, p-value = 0.2104 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.6112609 2.6475660 ## sample estimates: ## mean in group less mean in group more ## 11.23027 10.21212 9.5 ANOVA (дисперсионный анализ) ANOVA (ANalysis Of VAriance) – статистический критерий, распределение и ключевая статистика которого относятся к семейству F-распределений. Это то чуть скошенное влево распределение, уже сильно отличающееся от нормального. На этом распределении мы (точнее специально обученные программы) будут располагать F-значение, которое будет вычисляться (и это не имеет ничего общего с тем, как распределены переменные – к ним все еще действует допущение о нормальности или большог ообъема выборки). Этот критерий используется, когда зависимая переменная (ЗП) измерена в метрической (количественной) шкале. Независимые переменные (НП) могут бытькатегориальными, но ЗП всегда количественная, иначе чуда не произойдет. ANOVA используется тогда, когда число сравнений, которые нам нужно сделать, становится больше двух. Можно сделать и два сравнения с помощью ANOVA, но это будет бессмысленно, так как по сути это старый добрый т-тест. Нулевая и альтернативные гипотеза для ANOVA: \\(H_0\\): \\(\\mu_1 = \\mu_2 = ... =\\mu_n\\) \\(H_1\\): \\(\\mu_1 \\neq \\mu_2 \\neq ... \\neq \\mu_n\\) Как получается, что нам нужно сделать больше двух сравнений? 9.5.1 Факторы и уровни Фактором в линейных моделях и дисперсионном анализе называется независимая переменная. Как правило, мы применяем ANOVA для сравнения групп, поэтому эта независимая переменная – категориальная, она принимает конечное число значений. Значений НП – это те группы, которые мы сравниваем между собой, они называются уровнями НП или уровнями фактора. В зависимости от числа НП и уровней НП можно конкретизировать экспериментальный план и план для ANOVA 9.5.2 Почему он дисперсионный Чтобы перейти к анализу, надо немного разобраться, что это воообще за метод, и почему он называется анализом дисперсий, хотя сравниваются тут, как и в большинстве статистических критериев, средние значения по группам. Дело в том, что этот метод действительно учитывает дисперсию (хоть и не сравнивает ее между группами в качестве ключевого сравнения). Всю дисперсию можно разделить на межгрупповую и внутригрупповую. Если окажеется, что дисперсия между группами больше, чем внутри, то можно сделать вывод в пользу различий между группами. В качесте дисперсии для математики ANOVA обычно берется только числитель, без деления на число элементов в группе (а саму дисперсию мы считали здесь. Общая сумма квадратов складывается из межгрупповой суммы квадратов (SSE, Sum of Squares Explained или SSB, Sum of Squares Between groups) и внутригрупповой (SSR, Sum of Squares Random или SSW, Sum of Squares Within groups) \\(SST = SSE + SSR\\) Предположим, что у нас есть m групп по n наблюдений в каждой из них (для простоты возьмем равные по численности группы). Тогда общая сумма квадратов (дисперсия без знаменателя) равна: \\(SST = \\sum_{i=1}^{m \\times n} (x_i - \\bar x)^2\\) \\(df_{SST} = m \\times n-1\\), m * n – количество всех элементов во всех группах Как вычислить межгрупповую и внутригрупповую сумму квадратов? \\(SSE = \\sum_{j=1}^m (\\bar x_j - \\bar x)^2\\) \\(df_{SSE} = m-1\\), m – количество групп \\(SSR = \\sum_{j=1}^m\\sum_{i=1}^n (x_i - \\bar x_j)^2\\) \\(df_{SSR} = k-1\\), n – количество элементов во группе, m – число групп 9.5.3 Допущения для ANOVA Пройдем по алгоритму выбора статистического теста: https://miro.com/app/board/uXjVOxmKhr8=/?share_link_id=245423331470 Помимо основных отраженных ответвлений, для каждого критерия существует ряд допущений. Допущения (assumptions) – это утверждения про характер наших данных, без соблюдения которых параметрические тесты будут работать некорректно (поэтому их никто и не любит). Данные распределены нормально (или, если измеряемый признак является случайной велиичной, в группах больше 30 наблюдений) Дисперсии однородны (гомогенны) – проверяется с помощью теста Левеня (Levene’s Test of Homogeneity of Variance). 9.5.4 Непараметрические аналоги В случае, если допущения нормальности (помним, что это скорее про большое число наблюдений в выборке и визуально значительное отличие от гауссианы) и однородности дисперсий нарушаются, мы не можем использовать ANOVA, и нужно использовать непараметрические аналоги . 9.5.5 Просто ANOVA и с повторными измерениями Тут тоже все довольно просто: Если у нас независимые выборки, то мы используем ANOVA или его непараметрический аналог – тест Краскелла-Уоллиса. Если выборки зависимые, то мы используем ANOVA c повторными измерениями (repeated measures) или его непараметрический аналог – тест Фридмана. Обилие наименований может быть пугающим, но по сути, это один и тот же тест с небольшими поправками – просто в разных названиях увековечено больше статистиков! 9.5.6 Взимодействие факторов 9.5.7 Пост-хоки и множественные сравнения Если мы провели тест, сравнили полученное p-значение с \\(\\alpha\\), и оказалось, что p-значение &lt; \\(\\alpha\\), и мы можем отвергнуть нулевую гипотезу \\(H_0\\) в пользу альтернативной \\(H_1\\). Значит ли это, что отличаются средние во всех группах? Или может быть так, что \\(\\mu_1 = \\mu_2\\) и \\(\\mu_1 \\neq \\mu_3\\)? Может. АНОВА говорит, что в каких-то группах есть значимые отличия между средними, но не говорит, в каких именно. А чтобы узнать, в каких группах есть значимые отличия, нужно провести серию пост-хок (post hoc), апостериорных тестов. Это просто попарные т-тесты с поправкой на множественные сравнения. 9.5.8 Вычисление ANOVA и непараметрических аналогов Посмотрим опять на данные и поисследуем зависимость балла по математике от того, живут ли студенты в городах (переменная address значение U (urban)) или деревнях (переменная address значение R). student school sex age address famsize Pstatus Medu Fedu Mjob Fjob reason guardian traveltime studytime failures schoolsup famsup paid_mat activities nursery higher internet romantic famrel freetime goout Dalc Walc health absences_mat G1_mat G2_mat G3_mat paid_por absences_por G1_por G2_por G3_por G_mat G_por absences_mat_groups absences_por_groups id1 GP F 18 U GT3 A 4 4 at_home teacher course mother 2 2 0 yes no no no yes yes no no 4 3 4 1 1 3 6 5 6 6 no 4 0 11 11 5.666667 7.333333 middle less id2 GP F 17 U GT3 T 1 1 at_home other course father 1 2 0 no yes no no no yes yes no 5 3 3 1 1 3 4 5 5 6 no 2 9 11 11 5.333333 10.333333 less less id4 GP F 15 U GT3 T 4 2 health services home mother 1 3 0 no yes yes yes yes yes yes yes 3 2 2 1 1 5 2 15 14 15 no 0 14 14 14 14.666667 14.000000 less less id5 GP F 16 U GT3 T 3 3 other other home father 1 2 0 no yes yes no yes yes no no 4 3 2 1 2 5 4 6 10 10 no 0 11 13 13 8.666667 12.333333 less less id6 GP M 16 U LE3 T 4 3 services other reputation mother 1 2 0 no yes yes yes yes yes yes no 5 4 2 1 2 5 10 15 15 15 no 6 12 12 13 15.000000 12.333333 middle middle id7 GP M 16 U LE3 T 2 2 other other home mother 1 2 0 no no no no yes yes yes no 4 4 4 1 1 3 0 12 12 11 no 0 13 12 13 11.666667 12.666667 less less id8 GP F 17 U GT3 A 4 4 other teacher home mother 2 2 0 yes yes no no yes yes no no 4 1 4 1 1 1 6 6 5 6 no 2 10 13 13 5.666667 12.000000 middle less id9 GP M 15 U LE3 A 3 2 services other home mother 1 2 0 no yes yes no yes yes yes no 4 2 2 1 1 1 0 16 18 19 no 0 15 16 17 17.666667 16.000000 less less id10 GP M 15 U GT3 T 3 4 other other home mother 1 2 0 no yes yes yes yes yes yes no 5 5 1 1 1 5 0 14 15 15 no 0 12 12 13 14.666667 12.333333 less less id11 GP F 15 U GT3 T 4 4 teacher health reputation mother 1 2 0 no yes yes no yes yes yes no 3 3 3 1 2 2 0 10 8 9 no 2 14 14 14 9.000000 14.000000 less less library(&quot;ez&quot;) model_abs_mat_ez = ezANOVA(data = students, dv = G_mat, wid = student, between = absences_mat_groups) model_abs_mat_ez ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 absences_mat_groups 2 317 0.8568567 0.4254751 0.005376968 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 2 317 11.28622 1450.235 1.233501 0.2926653 Еще хорошую и более поробную главу (даже две) про ANOVA написал мой друг и коллега Антон Ангельгардт, если интересно углубиться, можно почитать его материал. https://angelgardt.github.io/SFDA2022/book/oneway-anova.html model_address_mat_ez = ezANOVA(data = students, dv = G_mat, wid = student, between = address) model_address_mat_ez ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 1 address 1 318 6.013395 0.01473483 * 0.01855909 ## ## $`Levene&#39;s Test for Homogeneity of Variance` ## DFn DFd SSn SSd F p p&lt;.05 ## 1 1 318 0.8014323 1412.745 0.1803973 0.6713193 students %&gt;% ggplot(aes(x=absences_mat_groups, y = G_mat)) + geom_boxplot(aes(fill = absences_mat_groups)) + # scale_fill_viridis(discrete=TRUE) + theme_minimal() students %&gt;% ggplot(aes(x=absences_mat_groups, y = G_mat)) + geom_violin(aes(fill = absences_mat_groups)) + geom_boxplot(aes(fill = absences_mat_groups), width=.1) + # scale_fill_viridis(discrete=TRUE) + theme_minimal() "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
