# Данные и типы шкал

## Измерение и переменная

Что значит измерить что-либо? Это значит, **привести в соответствие исследуемому признаку какое-либо значение на шкале**.

Что может примером признака? Что угодно, что нам нужно измерить в рамках исследования: количество кружек кофе в день, уровень концентрации, количество ошибок, время реакции, степень выгорания, количество выполненных задач, уровень нейротизма, рейтинг студента, количество детей в семье, температура и т.д.

**Признак**, который мы исследуем, по-другому называется **переменной**. С этим понятием мы будем сталкиваться постоянно в анализе данных. По сути, если мы посмотрим на табличку наблюдений, то любой столбец с измерениям -- это переменная. По строкам располагаются наблюдения, например, каждый новый человек из нашей выборки. Значение в  определенной колонке -- это **значение** переменной данного наблюдения.

Вернемся к кейсу с Никитой, которые изучает выгорание сотрудников ВУЗов. Сделаем табличку данных, которые мог намерить Никита.

```{r, eval=TRUE, message = FALSE}
library(tidyverse)
library(kableExtra)
teacher_number <- seq(1,30,1)
age <-  sample(22:60, size = 30, replace = T)
exp_years <- sample(1:8, size = 30, replace = T)
exp_scaled <- ifelse(exp_years >= 1 & exp_years <= 2, "от 1 до 2",
                     ifelse(exp_years > 2 & exp_years <= 5, "от 3 до 5",
                            ifelse(exp_years > 5, "больше 5", exp_years)))
burnout_MBI <- sample(19:70, size = 30, replace = T)
univer <-  rep(c("MSU", "HSE", "MSU", "RANEPA", "HSE", "RANEPA"),5)
burnout <- tibble(teacher_number, age, exp_years, exp_scaled, burnout_MBI, univer)
kable(burnout[1:10,])
```
<!--kable(burnout) %>% scroll_box(width = "100%")-->

*Что здесь будет являться переменными?*

В определении измерения помимо признака есть второе важное понятие -- это шкала.

**Шкала** -- это система измерения. Для того, чтобы мы все могли пользоваться одинаковыми единицами измерениями и не сходили с ума, мы, люди планеты Земля, пользуемся едиными шкалами. Всего их 4, и они бывают **метрические и неметрические** -- то есть, можем ли мы приложить измерительную линейку к ним или нет. В качестве измерительной линейки здесь имеется в виду любой условный прибор, в котором есть цена деления (сантиметр, грамм, секунда, штука).

## Количественные и неколичественные данные

Данные, измеренные метрическими шкалами -- это **количественные** данные (например, рост, вес, число заболевших, температура. То, что нельзя измерить метрическими шкалами (например, цвет глаз, самочувствие, уровень нейротизма, уровень образования) -- это **неколичественные** данные, которые могут носить разные названия: **категориальные**, иногда **качественные**. 

<span style="color: grey;">Иногда данные называют качественными в противоположность количественным, но это не совсем верно: разделение на количественные и качественные обычно применяется по отношению к типам исследований, где качественные исследования -- это, например, интервью или анализ блоков текста. Но в результате этого анализа у нас вполне могут получиться количественные переменные, например, количество раз, которые употреблялось то или иное слово, поэтому по отношению к данным, а не типам исследований, я рекомендую не использовать слово "качественные". </span>

С количественными данными попроще -- это все, что можно измерить метрической шкалой, условной линейкой.

Среди неколичественных данных встречаются два типа: категориальные и ранговые (порядковые) 

Эти данные мы подробно рассмотрим на шкалах.

## Типы шкал

Как мы уже поняли, разные данные относятся к разным шкалам. 

Разные шкалы обладают разной **измерительной мощностью** -- точностью, с которой мы измеряем признак. Один и тот же признак можно измерить с разной точностью: например, в зависимости от исследовательского вопроса, рост может быть выражен количественно в сантиметрах на интервале {0; ∞}, а может быть закодирован в виде {"меньше 150 см"; 150 см и больше}, если нас интересует только преобладание над определенной чертой. 

Всего существуют 4 шкалы, если располагать их снизу вверх по измерительной мощности: наименований, порядковая, интервальная, отношений.

Шкала | Описание                                       | Возможные операции | Примеры              | 
------| -----------------------------------------------| ------------------ | -------------------- | 
**Отношений (абсолютная)** | Количественная, есть абсолютный ноль, можно посчитать и *на сколько* больше или меньше, и *во сколько раз* | =, $\neq$, >, <, +, -, ×, ÷ | Рост, вес, число заболевших | 
**Интервальная (разности)** | Количественная, но нет абсоолютного нуля, можно посчитать  *на сколько* больше или меньше, но нельзя посчитать, *во сколько раз* | =, $\neq$, >, <, +, - | Температура в градусах Цельсия, времяисчисление по разным календарям | 
**Порядковая (ранговая, ординальная)** | Категориальная (качественная), можно установить "больше" или "меньше", но нельзя посчитать количественно, на сколько больше или меньше | =, $\neq$, >, < | Уровень образования, уровень нейротизма, спортивный рейтинг | 
**Наименований (номинальная)** | Категориальная (качественная), нельзя установить "больше" или "меньше" | =, $\neq$ | Пол, цвет, место жительства, название университета | 


## Непрерывные и дискретные данные

Видим, что самые богатые возможности для измерений у нас простираются в количественных шкалах -- на шкале отношений (она самая крутая) и интервальной шкале (она похуже и вообще, на самом деле, в исследованиях в нашей области встречается редко).

Количественные данные бывают дискретные, когда переменная принимает строго определенные значения, и непрерывные, когда может принимать какие угодно значения, до бесконечности или на заданном интервале.

Например, в нашем примере с исследованием выгорания переменная возраст (age) может принимать любые значения: преподаватели могут быть и возраста 25 лет, и 27.5 лет, и 31.666.. лет -- это все значения из *области допустимых значений для этой переменной*. Но если мы рассмотрим количество заболевших коронавирусом, то их никак не может быть 27.5 или 31.666.. -- заболевшие не выражаются в дробных долях от одного человека.

Важное понятие здесь -- **область допустимых значений**. У непрерывной переменной это всегда интервал, например {0;+∞}, у дисретных -- строго определенные значения, которые, тем не менее, могут тоже стремиться к бесконечности, например, {0;1;2;3;4;5;6;7;8...}

<!--chapter:end-->

# Вероятность и случайные величины 

## Случайная величина

Мы немного поговорили про переменные, они же -- признаки исследуего нами объекта (рост, вес, пол, уровень образования и так далее). Поговорили, что чтобы измерить признак, нужно привести в соответствие какое-либо значение из шкалы. Теперь давайте посмотрим на математический смысл переменных и их значений.

С математической точки зрения значения переменных являются **случайными величинами**. В теории вероятностей случайной величиной называется величина, которая *в данный момент времени (момент измерения) может принимать только одно значение, которое нельзя предугадать точно (до измерения)*.

Нам нужно замерить рост? Можем ли мы заранее сказать, сколько точно вплоть до микрометров он будет составлять? Если нет, значит, мы провели **испытание** (статистический термин единичного исследования или измерения), в ходе которого случайная величина рост приняла определенное значение (мы привели ей в соответствие какое-то значение из количественной шкалы). Говоря статистическим языком, наступило **событие** или **явление** "РОСТ = 178 см".

Нужно определить время реакции после выпитой кружки кофе? Все то же самое, время реакции -- случайная величина.

Результат прохождения опросника -- случайная величина.

Название ВУЗа, из которого пришел наш испытуемый -- случайная величина.

Это понятие нужно нам для того, чтобы мы могли считать **вероятности** наступления определенных **событий**, то есть наших измеренных переменных. Дело в том, что про вероятности случайных величин нам плюс-минус понятно и просто, а вот если величина перестает быть случайной, расчет вероятности становится сложнее. И мы пока рассматриваем только случайные величины.

## Вероятность

Что такое вероятность?

По сути, **вероятность** -- это численно выраженая возможность наступления того или иного события.

Вероятность может рассматриваться как **частота наступления** уже свершившегося события: по тому, как часто оно происходило, можно оценить, какова вероятность его наступления в дальнейшем.

Здесь начинается неожиданная развилка: в зависимости от того, как мы понимаем вероятность, приравниваем ли ее к частоте, статистика делится на **байесовскую (bayesian)** и **частотную (фреквинтистскую, frequentist)**. Статистика, в которой мы заменяем вероятности частотами, а не высчитываем вероятность по сложной формуле, и в которой мы будем работать -- частотная (https://en.wikipedia.org/wiki/Frequentist_inference). То есть говоря о вероятности, мы будем понимать ее исключительно так же, как и частоту, забываем про существование условной вероятности, формулу полной вероятности и других сложных концепций: если бы мы провели много раз одно и то же исследование, скажем, тысячу, и посмотрели, сколько раз в этом исследовании выпадает результат, который нас интересует, мы бы сказали, что вероятность наступления этого события -- это сколько раз оно выпадало из тысячи. И будем руководствоваться только этим смыслом.

Есть разные определения, в рамках статистики различают статистическое и геометрическое определение вероятностей. Можно углубиться на http://mathprofi.ru/sluchainaya_velichina.html. Разные определения вероятности используются в зависимости от того, с какими случайными величинами (СВ) мы работаем, различают два их типа: 
* **дискретные СВ**
* **непрерывные СВ**

## Дискретные СВ и статистическое определение вероятности

Самая часто используемая в теории вероятностей модель -- бросание обычного шестигранного игрального кубика. Кубик отличает то, что всего возможно наступление 6 событий (не будем рассматривать, что кубик смещенный или какой-то неправильный). То есть **область допустимых значений** или **область определения** для случайной величины "бросок кубика" это 6 значений: $ D \in \{1, 2, 3, 4, 5, 6\}$

Величина, область значений которой состоит из конечного числа натуральных чисел (1; 2; 3; 4; 5...), называется **дискретной**.

Для определения вероятности дискретной величны воспользуемся ее статистическим определением, не углубляясь в статистические термины: если при проведении испытания возможны $n$ равновероятных исходов значений случайной величины $A$, при этом в $m$ из них случается интересующее нас конкретное событие $A_{i}$,то *вероятность наступления события* $P(A_{i}) = \frac{m}{n}$ 

```{r echo= FALSE, fig.align = 'center', out.width="25%"}
knitr::include_graphics("docs/images/kubik.jpg")
```
<p align="center"> </p>

Бросание игрального кубика -- это *испытание*, выпадение одной из граней -- *исход*, а выпадение конкретно шестерки -- *событие*.

Обозначим выпадние грани в результате бросания кубика буквой $K$. Чему равна вероятность выпадения каждой грани $K_{1}$, $K_{2}$, $K_{3}$, $K_{4}$, $K_{5}$, $K_{6}$?

По статистическому определению вероятности: возможных исходов всего -- 6, интересующее нас событие случается в одном случае из 6, то есть:
$P(K_{1})$ = $P(K_{2})$ = $P(K_{3})$ = $P(K_{4})$ = $P(K_{5})$ = $P(K_{6})$ = $\frac{1}{6}$

Важно, что все события *равновероятны*. Если бы мы жили в мире с кубиками со смещенным центром тяжести, выпадние граней не было бы равновероятным. 

А чему равна **полная** вероятность?

Как можно вывести это математически из вероятностей наступления событий в бросании игрального кубика?
$P(K_{1})$ + $P(K_{2})$ + $P(K_{3})$ + $P(K_{4})$ + $P(K_{5})$ + $P(K_{6})$ = $1$

Единичные события нас мало интересуют (предмет изучения теории вероятностей -- массовые события), поэтому давайте представим, что мы бросили кубик несколько раз. Например, 20.

```{r}
library(TeachingDemos)
rolls <- dice(20,1)
t(rolls)
```

Построим таблицу частот выпадения каждой грани

```{r}
table(rolls)
```

```{r}
prop.table(table(rolls))
```

Можем наглядно посмотреть это на **гистограмме** -- графике, отображающем частоты встречаемости событий.

```{r}
rolls %>% 
  ggplot(aes(x=Red)) +
  geom_histogram(bins = 6, binwidth = 1) +
  theme_minimal()
```

Гистограмма -- это частный случай **столбчатой (или столбиковой) диаграммы**. Она отличается от столбчатой диаграммы тем, что, в отличие от нее, мы сами можем регулировать ширину столбика по оси $x$, задавать, сколько значений переменной х в него войдет, и высотка столбика будут пересчитана в зависимости от этого количества. В столбиковой диаграмме мы так сделать не можем, по оси $x$ всегда только одно значение, каждый столбик соотносится с конкретным значением переменной.

## Непрерывные СВ и геометрическое определение вероятности

С непрерывными величинами чуть посложнее, поэтому нам понадобится геометрическое определение вероятности.

Построим отрезок на оси $x$ от 0 до 1.
```{r echo= FALSE, fig.align = 'center', out.width="60%"}
knitr::include_graphics("docs/images/line.png")
```
<p align="center"> </p>


Допустим, мы проводим испытание, где бесконечно малым курсором проводим по этому отрезку. Остановку курсора в какой-то точке обозначим за $L$. Какова вероятность, что курсор остановится в точке с координатами $x=0.4857856$?

Сколько точек лежат в этом отрезке? Вспоминаем, что точка -- бесконечно малая величина, поэтому на этом отрезке лежит бесконечное число точек.
**Область допустимых значений** или **область определения** для случайной величины "остановка курсора в точке" -- бесконечное число значений в диапазоне $D \in [0;1]$

Величина, область допустиимых значений которой состоит из бесконечного числа значений на каком-либо ненеулевом промежутке, которые невозможно посчитать, называется **непрерывной**.

Попробуем воспользоваться статистическим определением вероятности:

$P(L_{.4857856}) = \frac{1}{множество всех точек на отрезке} = \frac{1}{\infty} \sim 0$

В пределе это число равно 0.

Получается, что мы не можем посчитать математически (статистически), и приходится прибегать к геометрическому определению.
Построим таблицу и график частот для значений из отрезка.
Для это сгенерируем набор из 10 случайных чисел в диапазоне [0;1].

```{r}
set.seed(42)
line_sample <- runif(10, min=0, max=1)
line_sample

table(line_sample)

prop.table(table(line_sample))
```
Видимо, что все значения встретились только один раз. Построим график. Можем построить гистограмму для этих значений:

```{r}
as_tibble(line_sample) %>% 
  ggplot(aes(x=value)) +
  geom_histogram(bins = 10) +
  theme_minimal()
```

Получается довольно странно: мы видели, что частота встречаемости разных значений -- всегда была единичка, а на гистограмме кажется по-другому. Так происходит, потому что мы взяли только 10 очень конкретных крошечных значений из непрерывной величины, и нам попались какие-то значения -- рядом друг с другом, и столбики для их частот "слиплись" в один большой столбик, и частота получилась не 1, а 2. А какие-то значения попались далеко друг от друга, поэтому их столбики слиплись не друг с другом, а с более близкими к ним столбиками, и на месте частоты для этого значения образовалась дырка.

Такой график для непрерывной величины не очень верный: чтобы стобики не "слипались" и не обманывали нас визуально, будто где-то в значениях есть дыра, для непрерывных величин мы будем использовать другой график -- **график плотности вероятности (probability density)**
```{r}
as_tibble(line_sample) %>% 
  ggplot(aes(x=value)) +
  geom_density() +
  theme_minimal()
```
Он тоже показывает так, будто какие-то значения более частые, какие-то менее -- но ситуация уже гораздо лучше. Здесь мы сгенерировали 10 значений, а если нам вдруг нужно визуализировать все возможные значения из отрезка [0;1], а х бесконечность? Тогда гистограмма вообще потеряет всякий смысл, так как разбивает все значения на столбики из конечных интервалов, а вот непрерывная линию будет подходящей -- в непрерывной линии бесконечное число значений! То есть выходит следующая логика: для визуализации дискретных величин обычно используют гистограмму, для визуализации непрерывных -- график плотности вероятности.

Почему **плотности вероятности**, а не самой вероятности? Потому что как мы вывели выше, если бы мы считали точно именно вероятность, она бы стремилась к нулю. А засчет того, что это именно плотность, буквально, "тут побольше значений", "тут поменьше значений" -- получается осмысленный график. То есть, простыми словами, плотность вероятности -- характер распределения вероятностей в самом значении и его окрестности. Если в интервале [0.25;0.35] оказалось (чисто случайно) мало значений, а в интервале [0.58;0.65] оказалось побольше -- то на графике плотности вероятности мы увидим различия в вероятности для этих интервалов, хотя вероятность встретить каждое конкретное очень точно заданное число (например, 0.4857856) стремится к 0.
Мы не будем рассматривать математически смысл плотности вероятности, но функция плотности вероятности является производной от функции самой вероятности.

Попробуем сгенерировать этот же график для 500 и 10 000 точек и посмотреть, к чему будет стремиться график плотности вероятности в случае, когда мы случайно генерируем множество значений, все из которых равновероятны?

```{r}
line_sample_500 <- runif(500, min=0, max=1)

as_tibble(line_sample_500) %>% 
  ggplot(aes(x=value)) +
  geom_density() +
  theme_minimal()
```
10000 точек из этого же диапазона
```{r}
line_sample_10000 <- runif(10000, min=0, max=1)

as_tibble(line_sample_10000) %>% 
  ggplot(aes(x=value)) +
  geom_density() +
  theme_minimal()
```

> Итак, запомнили, что мы рассматриваем признаки (переменные) как случайные величины, они могут быть дисркетные и непрерывные, и их вероятности описываются определенными законами распределения.

# Распределения

Что такое распределение? Мы постоянно употребляем это слово.

Говоря о распределениях, мы имеем в виду **закон распределения случайной величины** – соответствие между возможными значениями этой величины из области ее допустимых значений и: вероятностями этих значений -- для дискретной величины, либо плотностью вероятности -- для непрерывной величины.

Законы распределений вероятностей можно вывести совсем не для всех величин! Но так сложилось, что некоторые закономерности, распределения вероятностей мы можем описать формулами, как в физике: сила тяжести (сила, с которой Земля притягивает все теля) прямо пропорциональная массе объекта, взятая с коэффициентом ускорения свободного падения $F = m*g$. Мы это заметили относительно окружающего мира и вывели закон (не лично мы, а вообще представители планеты Земля). Точно так же и с распределниями: мы заметили, что вероятности распределения некоторых случайных величин подчиняются определенным законам, и записали их. Например, доказано, что непрерывные случайные величины, на которые действует множество случайных факторов (например, рост, вес и тд), распределяются в соответствии с **распределением Гаусса**, оно же -- **нормальное распределение**. Его формула:

$P(x) = \frac{e^{-(x - \mu)^{2}/(2\sigma^{2}) }} {\sigma\sqrt{2\pi}}$

```{r}
plot(dnorm(1:1000, mean = 500, sd = 100))
```

```{r, echo=F, eval=F}
# dist_sample <- sample(1:1000, 1000, replace = T)

normdist_sample <- dnorm(1:1000, 500, 100)
as_tibble(normdist_sample) %>% 
  ggplot(aes(x=1:1000, y = normdist_sample)) +
  geom_density() +
  theme_minimal()
```

Или, например, экспоненциальное распределение:

$P(x)= \lambda \times e^{-\lambda x}$

```{r}
plot(dexp(x = 1:100, rate = 0.1))
```
χ2-распределение:

$P(x)= \lambda \times e^{-\lambda x}$

```{r}
plot(dchisq(1:1000, df=100))
```


Или, например, биномиальное распределение:

$P(k | n, p) = \frac{n!}{k!(n-k)!} \times p^k \times (1-p)^{n-k} =  {n \choose k} \times p^k \times (1-p)^{n-k}$

```{r}
plot(dbinom(1:100, size=100, prob=0.5))
```
Посмотреть и ужаснуться можно тут http://www.math.wm.edu/~leemis/chart/UDR/UDR.html. Нам, к счастью, ничего из этого не понадобится.

Менее пугающая версия https://www.johndcook.com/blog/distribution_chart/#normal

На примере данных про выгорание:

```{r}
kable(burnout)
```


```{r}
burnout %>% 
  ggplot(aes(x=exp_years)) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal()
```



```{r}
burnout %>% 
  ggplot(aes(x=age)) +
  geom_density() +
  theme_minimal()
```


## Функции распределения

Выше мы познакомились с плотностью вероятностью, зафиксируем все применимые к вероятности функции, которые бывают полезны:

* **функция плотности вероятности (probability density function)** для непрерывных СВ (например, рост, вес) и **функция вероятности (probability mass function)** для дискретных СВ (например, количество заболевших) -- самая простая базовая функция, часто обозначается буквой `d*`
* **функция накопленной вероятности / плотности вероятности (cumulative distribution function)** для непреревных СВ, часто обозначается буквой `p*`
* **квантильная функция (quantile function)**, она же обратная функция накопленной плотности распределения, об этом попозже, часто обозначается буквой `q*`
**Функция плотности вероятности (cumulative distribution function; cdf)**

Зачем они нужны?

Разберемся на примере тестов на IQ.

**Функция плотности вероятности (probability density function)**
```{r}
iq <- seq(50,150, 0.1)
plot(iq, dnorm(iq, mean = 100, sd = 15))
```

**Функция накопленной плотности  (cumulative distribution function; cdf)**

```{r}
plot(iq, pnorm(iq, mean = 100, sd = 15))
```

<!--chapter:end-->
