# Распределения и случайные величины 

## Случайная величина

Мы немного поговорили про переменные, они же -- признаки исследуего нами объекта (рост, вес, пол, уровень образования и так далее). Поговорили, что чтобы измерить признак, нужно привести в соответствие какое-либо значение из шкалы. Теперь давайте посмотрим на математический смысл переменных и их значений.

С математической точки зрения значения переменных являются **случайными величинами**. В теории вероятностей случайной величиной называется величина, которая *в данный момент времени (момент измерения) может принимать только одно значение, определенное стечением случайных обстоятельств,  которое нельзя предугадать точно*.

Нам нужно замерить рост? Можем ли мы заранее сказать, сколько точно вплоть до микрометров он будет составлять? Влияют ли на него прямо сейчас какие-то факторы? Если нет, значит, мы провели **испытание** (статистический термин единичного исследования или измерения), в ходе которого случайная величина рост приняла определенное значение (мы привели ей в соответствие какое-то значение из количественной шкалы). Говоря статистическим языком, наступило **событие** или **явление** "РОСТ = 178 см".

Нужно определить время реакции после выпитой кружки кофе? Все то же самое, время реакции -- случайная величина.

Результат прохождения опросника -- случайная величина.

Название ВУЗа, из которого пришел наш испытуемый -- случайная величина.

Это понятие нужно нам для того, чтобы мы могли считать **вероятности** наступления определенных **событий**, то есть наших измеренных переменных. Дело в том, что про вероятности случайных величин нам плюс-минус понятно и просто, а вот если величина перестает быть случайной, расчет вероятности становится сложнее. И мы пока рассматриваем только случайные величины.

## Вероятность

Что такое вероятность?

Есть разные определения, в рамках статистики различают статистическое и геометрическое определение вероятностей. Можно углубиться на http://mathprofi.ru/sluchainaya_velichina.html Пока воспользуемся статистическим определением, не углубляясь в статистические термины: если при проведении испытания возможны $n$ равновероятных исходов значений случайной величины $A$, при этом в $m$ из них случается интересующее нас конкретное событие $A_{i}$,то *вероятность наступления события* $P(A_{i}) = \frac{m}{n}$ 

## Дискретные случайные величины и гистограмма

Самая часто используемая в теории вероятностей модель -- бросание игрального кубика. 

```{r echo= FALSE, fig.align = 'center', out.width="30%"}
knitr::include_graphics("docs/images/kubik.jpg")
```
<p align="center"> </p>

Бросание игрального кубика -- это *испытание*, выпадение одной из граний -- *исход*, а выпадение конкретно шестерки -- *событие*.

Обозначим выпадние грани в результате бросания кубика буквой $K$. Чему равна вероятность выпадения каждой грани $K_{1}$, $K_{2}$, $K_{3}$, $K_{4}$, $K_{5}$, $K_{6}$?

По статистическому определению вероятности: возможных исходов всего -- 6, интересующее нас событие случается в одном случае из 6, то есть:
$P(K_{1})$ = $P(K_{2})$ = $P(K_{3})$ = $P(K_{4})$ = $P(K_{5})$ = $P(K_{6})$ = $\frac{1}{6}$

Важно, что все события *равновероятны*. Если бы мы жили в мире с кубиками со смещенным центром тяжести, выпадние граней не было бы равновероятным. 

А чему равна **полная** вероятность?

Как можно вывести это математически из вероятностей наступления событий в бросании игрального кубика?
$P(K_{1})$ + $P(K_{2})$ + $P(K_{3})$ + $P(K_{4})$ + $P(K_{5})$ + $P(K_{6})$ = $1$

Единичные события нас мало интересуют (предмет изучения теории вероятностей -- массовые события), поэтому давайте представим, что мы бросили кубик несколько раз. Например, 20.

```{r}
library(TeachingDemos)
rolls <- dice(20,1)
rolls
```

Построим таблицу частот выпадения каждой грани

```{r}
table(rolls)
```

```{r}
prop.table(table(rolls))
```

Можем наглядно посмотреть это на **гистограмме** -- графике, отобрающем частоты встречаемости событий.

```{r}
rolls %>% 
  ggplot(aes(x=Red)) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal()
```

Гистограмма -- это частный случай **столбиковой диаграммы**.


## Непрерывные случайные величины и плотность вероятности

С непрерывными величинами чуть посложнее, поэтому нам понадобится геометрическое определение вероятности.

Построим отрезок на оси $x$ от 0 до 1.
```{r echo= FALSE, fig.align = 'center', out.width="60%"}
knitr::include_graphics("docs/images/line.png")
```
<p align="center"> </p>

Допустим, мы проводим испытание, где бесконечно малым курсором проводим по этому отрезку. Остановка курсора в какой-то точке обозначим за $L$. Какова вероятность, что курсор остановится в точке с координатами $x=0.4857856$?

$P(L_{.4857856}) = \frac{1}{множество всех точек на отрезке} = \frac{1}{\infty} \sim 0$

В пределе это число равно 0.

Получается, что мы не можем посчитать математически (статистически), и приходится прибегать к геометрическому определению.
Построим таблицу и график частот для значений из отрезка

```{r}
line_sample <- runif(10, min=0, max=1)
line_sample

table(line_sample)

plot(density(line_sample))

```



```{r}
line_sample <- runif(500, min=0, max=1)
line_sample

table(line_sample)

plot(density(line_sample))
```
Это график **плотности вероятности (probability density)**. 

Мы не можем построить график частот для непрерывных случайных величин -- они все будут равны единице. но мы можем как бы объединить близ лежащие очки и примерно оценить встречаемость этих точек в нашей выборке. Если мы нарисуем такой график, то мы можем рассчитыать **вероятность как площадь под кривой графика плотности вероятности.**

**Закон распределения случайной величины** – это соответствие между возможными значениями этой величины и их вероятностями.

> Итак, запомнили, что мы рассматриваем признаки (переменные) как случайные величины, они могут быть дисркетные и непрерывные, и их вероятности описываются определенными законами распределения.

## Распределения

**Функция плотности вероятности (cumulative distribution function; cdf)**

```{r}
iq <- seq(50,150, 0.1)
plot(iq, dnorm(iq, mean = 100, sd = 15))
```

**Функция накопленной плотности  (cumulative distribution function; cdf)**

```{r}
plot(iq, pnorm(iq, mean = 100, sd = 15))
```


Здесь начинается неожиданная развилка: в зависимости от того, как мы понимаем вероятность, статистика делится на байесовскую и частотную (фреквинтистскую, frequentist). Статистика, которую мы сейчас обсуждаем, называется фреквентистской (частотной) https://en.wikipedia.org/wiki/Frequentist_inference

## Распределения

Выборочное распределение и распределения для генеральной совокупности.



```{r}
kable(burnout)
```


```{r}
burnout %>% 
  ggplot(aes(x=exp_years)) +
  geom_histogram(binwidth = 0.5) +
  theme_minimal()
```



```{r}
burnout %>% 
  ggplot(aes(x=age)) +
  geom_density() +
  theme_minimal()
```

```{r}
tibble(x = 1:100,
           PDF = dnorm(x = x, mean = 50, sd = 10)) %>% 
  ggplot(aes(x, PDF))+
  geom_point()+
  labs(title = "PDF нормального распределения (μ = 50, sd = 10)")
```


## Распределения



Посмотреть и ужаснуться можно тут http://www.math.wm.edu/~leemis/chart/UDR/UDR.html. Нам, к счастью, ничего из этого не понадобится.

Менее пугающая версия https://www.johndcook.com/blog/distribution_chart/#normal


### Нормальное распределение

$P(x) = \frac{e^{-(x - \mu)^{2}/(2\sigma^{2}) }} {\sigma\sqrt{2\pi}}$

### Другие распределения


Биномиальное распределение

$P(k | n, p) = \frac{n!}{k!(n-k)!} \times p^k \times (1-p)^{n-k} =  {n \choose k} \times p^k \times (1-p)^{n-k}$

```{r}
tibble(x = 0:50,
           density = dbinom(x = x, size = 50, prob = 0.16)) %>% 
  ggplot(aes(x, density))+
  geom_point()+
  geom_line()+
  labs(title = "Биномиальное распределение p = 0.16, n = 50") +
  theme_minimal()
```

Экспоненциальное распределение

$ P(x)= \lambda \times e^{-\lambda x}$

```{r}
tibble(x = 1:20,
           PDF = dexp(x = x, rate = 0.5)) %>% 
  ggplot(aes(x, PDF))+
  geom_point()+
  geom_line()+
  labs(title = "PDF экспоненциального распредления с коэффициентом и")
```







