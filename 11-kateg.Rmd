#  Анализ категориальных данных {#stats_criteria_categ} 

```{r, eval=TRUE, echo = FALSE, message = FALSE}
library(tidyverse)
library(kableExtra)
library(viridis)
studens_mat <- read_csv("student-mat.csv") %>% 
  rename_with(., ~ paste0(., "_mat"), .cols = c(absences, paid, G1, G2, G3)) -> studens_mat 
studens_por <- read_csv("student-por.csv") %>% 
  rename_with(., ~ paste0(., "_por"), .cols = c(absences, paid, G1, G2, G3)) -> studens_por
studens_mat %>% 
  full_join(studens_por, by = c("school","sex","age","address","famsize","Pstatus","Medu","Fedu",
                             "Mjob","Fjob","reason", "guardian", "traveltime","studytime", "failures", "schoolsup", "famsup",
                             "activities", "nursery", "higher", "internet", "romantic", "famrel", "freetime", "goout", 
                             "Dalc", "Walc", "health")) -> students 

students %>% 
  mutate("student" = paste0("id", row_number()), .before = "school")  %>% 
  drop_na() %>% 
  mutate(G_mat = rowMeans(dplyr::select(., c(G1_mat, G2_mat, G3_mat))),
         G_por = rowMeans(dplyr::select(., c(G1_por, G2_por, G3_por)))) %>% 
  mutate(absences_mat_groups = ifelse(absences_mat <=5, "less", ifelse(absences_mat <=15, "middle", "more"))) %>% 
  mutate(absences_por_groups = ifelse(absences_por <=5, "less", ifelse(absences_por <=15, "middle", "more"))) -> students
```

До этого мы все время работали с линейными моделями, где целевая (зависимая) переменная была количественной. И линейные регрессионные модели, и ANOVA модели относятся к **общим линейным омделям (general linear models)**. Но что делать, если цеелвая переменнаая -- категориальная? Уже нельзя строить линейные модели? Общие линейные -- нет, а обобщенные -- можно! Модели с категориальными предикторами включаются в обобщенные линейные модели generalized linear modela (не перепутайте...)

## Логистическая регрессия {#logreg} 

К логистической регрессии относится семейство моделей, где целевая переменная может принимать значения 1 или 0. То есть она принадлежит [распределению Бернулли](#distributions). Можно поизучать его на шайниапп, котрый сделал Антон Ангельгардт https://angelgardt.shinyapps.io/binomial_app/

Но если мы посмотрим на классическое уравнение регрессии, можем заметить проблему:

<p align="center">$\hat y = b_0 + b_1 \times x_1$  </p>

В левой части уравнения у нас стоят только значения $\hat y={1;0}$, когда как в правой -- множество непрерывных значений от минус бесконечности до бесконечности.

Чтобы ее решить, перейдем сначала к вероятностям: вместо значений 0 и 1 будем использовать вероятность для, например, 1. 

<p align="center">$p_1= b_0 + b_1 \times x_1$,</p>
<p align="center">$p_1 \in [0;1]$</p>

Уже лучше, но мы все еще остались в границах от 0 до 1. Чтобы все-таки превратить это уравнение в уже привычную нам линейную регрессию с количественной ЗП, сделаем *логарифмическую трансформацию шансов* -- перейдем от вероятности к шансам для каждого исхода, 0 и 1, и посчитаем логарифм от них. Вероятность наступления исхода тогда станет обратной этому логарифму величиной. 

<p align="center">$odds_1 = \frac{p_1}{p_0} = \frac{p_1}{1 - p_1}$,</p>
<p align="center">$p_1 \in [0;1]$</p>

<p align="center">$ln(odds_1) = logit(p_1) = ln(\frac{p_1}{1 - p_1}) = b_0 + b_1 \times x_1$,</p>
<p align="center">$p_1 = \frac{e^{b_0 + b_1 x}}{1 + e^{b_0 + b_1 x}}$,</p>
<p align="center">$ln(\frac{p_1}{1 - p_1}) \in (-\infty;\infty)$</p>

Здесь сильно упрощена математика, вслед за [курсом Анатолия Карпова на степике](https://stepik.org/course/524) не будем вдаваться в подробности. Здесь важно, что мы постарались применить одинаковое преобразование к обеим частям уравнения, поэтому вероятность теперь считается по функции с экспонентой (эта функция, к слову, очень важна в машинном обучении и называется сигма-функцией или сигмоидой из-за ее вида) -- то есть, по определению логарифма, это показатель степени, в которую нужно возвести основание логарифма, чтобы получить данное число ($ln(p_1) = b_0 + b_1 \times x_1$).

<p align="center"> 
```{r eval=TRUE, echo = FALSE, message = FALSE, fig.align = 'center', out.width="80%"}
knitr::include_graphics("docs/images/Logistic-curve.svg.png")
```

Сначала построим просто модель только с интерсептом, которая будет показывать, каковы шансы, что студент заплатит за курс на Udemy сами по себе, без учета каких-либо переменных -- то есть, по сути, это тот же самый анализ, что и хи-квадрат -- мы берем только частоты и строим на них модель.

```{r, eval=TRUE, echo = FALSE, message = FALSE}
library(tidyverse)
library(kableExtra)
library(viridis)
udemy <- read_csv("Course_info.csv")
udemy %>% 
  select(!headline) -> udemy
udemy_sample <- sample_n(udemy, 1000, replace = T)
udemy_sample %>% 
  filter(num_subscribers > 1 & price > 0 & content_length_min>0 & num_lectures>0) %>% 
  mutate(price_log = log(price), num_subscribers_log = log(num_subscribers)) %>% 
    filter(content_length_min<300 & num_lectures<80 & num_subscribers < 5000 & avg_rating >=3 & price > 1 & price<200) -> udemy_sample
```

! Важный момент! R считает за референс (первое значение, то, что будет в числителе) всегда наименьшее значение, то есть шансы будут считаться для 0, а не для 1. Если мы имеем дело с не числовыми значениями, то можем перезадать их с помощью функции relevel().


```{r, eval=TRUE, echo = TRUE, message = FALSE,warning=FALSE}
udemy_sample %>% 
  glm(is_paid ~1, ., family = binomial) ->logmodel1
summary(logmodel1)
```
Интерсепт здесь получился равен `r coef(logmodel1)` -- и есть натуральный логарифм шансов для оплаты. Посчитаем саму вероятность:
$p = \frac{exp^{26.56}}{1+exp^{26.56}}$

Это число получилось равно 692204438686. То есть, шансы, что студенты НЕ заплят за курс (0) весьма высокие...

Теперь посчитаем, как влияет на оплату цена курса


```{r, eval=TRUE, echo = TRUE, message = FALSE, warning=FALSE}
udemy_sample %>% 
  glm(is_paid ~ price_log, ., family = binomial) ->logmodel2
summary(logmodel2)
```
## Хи-квадрат

Хи-квадратом мы обычно проверяем коррялцию категориальных переменных между собой: порядковых, если у нас не шкала Лайерта или мало возможных значений, и метод корреляции Спирмена не подходит, или номинальных, где корреляцию Спирмена провести уже невозможно.

Например, давайте проверим гипотезу о том, что наличие или отсутствие интернета в квартире связано с наличием или отсутствием романтических отношений. Предположим, что португальские школьники, у кого дома есть интернет, реже вступают в романтические отношения, по сравнению с теми, у кого интернета нет. Для этого будем использовать метод хи-квадрат. НП – наличие или отсутствие интернета (переменная internet), ЗП – наличие или отсутствие романтических отношений (romantic).

```{r, eval=TRUE, echo = TRUE, message = FALSE}
chisq.test(students$internet, students$romantic, correct=FALSE)
```
Сравниваем полученное p-value с выбранным уровнем  α=0.05. p-value < α, значит, вероятность получить такую связь между переменными при условии, если на самом деле ее нет, меньше нашего допустимого уровня ложноположительных результатов, мы отвергаем нулевую гипотезу и делаем вывод о наличии корреляционной связи.
 
Хи-квадрат чаще всего визуализируют уже знакомым по корреляции Спирмена мозаичным графиком. Указываем НП как x=product(НП), и ЗП как fill = ЗП.
 
```{r, eval=TRUE, echo = TRUE, message = FALSE, warning=FALSE}
library(ggmosaic)
students %>% 
  ggplot(aes()) + 
  geom_mosaic(aes(x = product(internet), fill = romantic)) +
  scale_fill_viridis(discrete=TRUE) +
  theme_minimal() 
```
